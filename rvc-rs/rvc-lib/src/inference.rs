//! RVC æ¨ç†ç®¡é“
//!
//! è¯¥æ¨¡å—å®ç°äº†å®Œæ•´çš„ RVC (Retrieval-based Voice Conversion) æ¨ç†æµç¨‹ï¼Œ
//! æ•´åˆäº† HuBERT ç‰¹å¾æå–ã€F0 ä¼°è®¡ã€FAISS æ£€ç´¢å’Œç”Ÿæˆå™¨ç½‘ç»œç­‰ç»„ä»¶ã€‚

use crate::{
    audio_utils::{AudioData, load_wav_simple, save_wav_simple},
    f0_estimation::{F0Config, F0Estimator, F0Method},
    faiss_index::{FaissIndex, SearchResult},
    generator::{GeneratorConfig, NSFHiFiGANGenerator},
    hubert::{HuBERT, HuBERTConfig},
};

use anyhow::Result;
use ndarray::{Array2, ArrayView2};
use std::path::Path;
use tch::{Device, Kind, Tensor, nn};

/// RVC æ¨ç†é…ç½®
#[derive(Debug, Clone)]
pub struct InferenceConfig {
    /// ç›®æ ‡è¯´è¯äºº ID
    pub speaker_id: i64,
    /// F0 ä¼°è®¡æ–¹æ³•
    pub f0_method: F0Method,
    /// éŸ³é«˜è°ƒæ•´æ¯”ä¾‹ (1.0 = æ— è°ƒæ•´)
    pub pitch_shift: f64,
    /// ç‰¹å¾æ£€ç´¢æ··åˆæ¯”ä¾‹ (0.0-1.0)
    pub index_rate: f64,
    /// è¾“å‡ºéŸ³é¢‘é‡‡æ ·ç‡
    pub target_sample_rate: i64,
    /// è®¾å¤‡ (CPU/GPU)
    pub device: Device,
    /// æ‰¹å¤„ç†å¤§å°
    pub batch_size: usize,
    /// æ˜¯å¦å¯ç”¨å»å™ª
    pub enable_denoise: bool,
    /// F0 æ»¤æ³¢å™¨å‚æ•°
    pub f0_filter: F0FilterConfig,
}

/// F0 æ»¤æ³¢å™¨é…ç½®
#[derive(Debug, Clone)]
pub struct F0FilterConfig {
    /// ä¸­å€¼æ»¤æ³¢å™¨çª—å£å¤§å°
    pub median_filter_radius: usize,
    /// æ˜¯å¦å¯ç”¨ F0 å¹³æ»‘
    pub enable_smoothing: bool,
    /// å¹³æ»‘å‚æ•°
    pub smoothing_factor: f64,
}

impl Default for InferenceConfig {
    fn default() -> Self {
        Self {
            speaker_id: 0,
            f0_method: F0Method::Harvest,
            pitch_shift: 1.0,
            index_rate: 0.5,
            target_sample_rate: 22050,
            device: Device::Cpu,
            batch_size: 1,
            enable_denoise: false,
            f0_filter: F0FilterConfig::default(),
        }
    }
}

impl Default for F0FilterConfig {
    fn default() -> Self {
        Self {
            median_filter_radius: 3,
            enable_smoothing: true,
            smoothing_factor: 0.8,
        }
    }
}

/// RVC æ¨ç†å¼•æ“
pub struct RVCInference {
    config: InferenceConfig,
    hubert: HuBERT,
    generator: NSFHiFiGANGenerator,
    f0_estimator: F0Estimator,
    index: Option<FaissIndex>,
    vs: nn::VarStore,
}

impl RVCInference {
    /// åˆ›å»ºæ–°çš„æ¨ç†å¼•æ“
    pub fn new(
        config: InferenceConfig,
        model_path: impl AsRef<Path>,
        index_path: Option<impl AsRef<Path>>,
    ) -> Result<Self> {
        let vs = nn::VarStore::new(config.device);

        // åˆå§‹åŒ– HuBERT
        let hubert_config = HuBERTConfig::default();
        let hubert = HuBERT::new(&vs.root(), hubert_config, config.device);

        // åˆå§‹åŒ–ç”Ÿæˆå™¨
        let generator_config = GeneratorConfig::default();
        let generator = NSFHiFiGANGenerator::new(&vs.root(), generator_config);

        // åˆå§‹åŒ– F0 ä¼°è®¡å™¨
        let f0_config = F0Config {
            f0_min: 50.0,
            f0_max: 1100.0,
            ..Default::default()
        };
        let f0_estimator = F0Estimator::new(f0_config, config.device);

        // åŠ è½½ FAISS ç´¢å¼• (å¯é€‰)
        let index = if let Some(index_path) = index_path {
            match FaissIndex::load(index_path) {
                Ok(idx) => {
                    println!("âœ… FAISS ç´¢å¼•åŠ è½½æˆåŠŸ");
                    Some(idx)
                }
                Err(e) => {
                    println!("âš ï¸  FAISS ç´¢å¼•åŠ è½½å¤±è´¥: {}", e);
                    None
                }
            }
        } else {
            None
        };

        // åŠ è½½æ¨¡å‹æƒé‡
        if model_path.as_ref().exists() {
            println!("ğŸ“ æ­£åœ¨åŠ è½½æ¨¡å‹: {:?}", model_path.as_ref());
            // TODO: å®ç°æ¨¡å‹åŠ è½½é€»è¾‘
            println!("âš ï¸  æ¨¡å‹åŠ è½½åŠŸèƒ½æš‚æœªå®Œå…¨å®ç°ï¼Œä½¿ç”¨éšæœºæƒé‡");
        }

        Ok(Self {
            config,
            hubert,
            generator,
            f0_estimator,
            index,
            vs,
        })
    }

    /// æ‰§è¡Œè¯­éŸ³è½¬æ¢æ¨ç†
    pub fn convert_voice(
        &self,
        input_audio_path: impl AsRef<Path>,
        output_audio_path: impl AsRef<Path>,
    ) -> Result<AudioData> {
        println!("ğŸµ å¼€å§‹è¯­éŸ³è½¬æ¢æ¨ç†...");

        // 1. åŠ è½½è¾“å…¥éŸ³é¢‘
        let path_str = input_audio_path.as_ref().to_string_lossy();
        let audio_data =
            load_wav_simple(&path_str).map_err(|e| anyhow::anyhow!("éŸ³é¢‘åŠ è½½å¤±è´¥: {}", e))?;
        println!(
            "ğŸ“Š è¾“å…¥éŸ³é¢‘: {}Hz, {} æ ·æœ¬",
            audio_data.sample_rate,
            audio_data.samples.len()
        );

        self.convert_audio_data(audio_data, Some(output_audio_path))
    }

    /// å¯¹éŸ³é¢‘æ•°æ®æ‰§è¡Œè½¬æ¢
    pub fn convert_audio_data(
        &self,
        mut audio_data: AudioData,
        output_path: Option<impl AsRef<Path>>,
    ) -> Result<AudioData> {
        // 2. é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if audio_data.sample_rate != self.config.target_sample_rate as u32 {
            println!(
                "ğŸ”„ é‡é‡‡æ ·: {}Hz -> {}Hz",
                audio_data.sample_rate, self.config.target_sample_rate
            );
            audio_data = self.resample_audio(audio_data)?;
        }

        // 3. é¢„å¤„ç†éŸ³é¢‘
        let processed_audio = self.preprocess_audio(&audio_data)?;

        // 4. HuBERT ç‰¹å¾æå–
        println!("ğŸ§  æå– HuBERT ç‰¹å¾...");
        let features = self.extract_hubert_features(&processed_audio)?;

        // 5. F0 ä¼°è®¡
        println!("ğŸ¼ ä¼°è®¡åŸºé¢‘ (F0)...");
        let f0_data = self.estimate_f0(&processed_audio)?;

        // 6. F0 åå¤„ç†
        let processed_f0 = self.postprocess_f0(f0_data)?;

        // 7. ç‰¹å¾æ£€ç´¢ (å¦‚æœå¯ç”¨)
        let enhanced_features = if let Some(ref index) = self.index {
            println!("ğŸ” æ‰§è¡Œç‰¹å¾æ£€ç´¢...");
            self.retrieve_features(&features, index)?
        } else {
            features
        };

        // 8. ç”Ÿæˆå™¨æ¨ç†
        println!("ğŸ¨ ç”ŸæˆéŸ³é¢‘æ³¢å½¢...");
        let generated_audio = self.generate_audio(&enhanced_features, &processed_f0)?;

        // 9. åå¤„ç†
        let final_audio = self.postprocess_audio(generated_audio)?;

        // 10. ä¿å­˜è¾“å‡º (å¦‚æœæŒ‡å®šè·¯å¾„)
        if let Some(output_path) = output_path {
            let path_str = output_path.as_ref().to_string_lossy();
            save_wav_simple(&path_str, &final_audio)
                .map_err(|e| anyhow::anyhow!("éŸ³é¢‘ä¿å­˜å¤±è´¥: {}", e))?;
            println!("ğŸ’¾ è¾“å‡ºå·²ä¿å­˜: {:?}", output_path.as_ref());
        }

        println!("âœ… è¯­éŸ³è½¬æ¢å®Œæˆ!");
        Ok(final_audio)
    }

    /// é‡é‡‡æ ·éŸ³é¢‘
    fn resample_audio(&self, audio_data: AudioData) -> Result<AudioData> {
        // ç®€å•çš„çº¿æ€§æ’å€¼é‡é‡‡æ · (ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ›´é«˜è´¨é‡çš„é‡é‡‡æ ·ç®—æ³•)
        let input_rate = audio_data.sample_rate as f32;
        let output_rate = self.config.target_sample_rate as f32;
        let ratio = output_rate / input_rate;

        let new_length = (audio_data.samples.len() as f32 * ratio) as usize;
        let mut resampled = Vec::with_capacity(new_length);

        for i in 0..new_length {
            let src_index = i as f32 / ratio;
            let idx = src_index as usize;

            if idx + 1 < audio_data.samples.len() {
                let frac = src_index - idx as f32;
                let sample =
                    audio_data.samples[idx] * (1.0 - frac) + audio_data.samples[idx + 1] * frac;
                resampled.push(sample);
            } else if idx < audio_data.samples.len() {
                resampled.push(audio_data.samples[idx]);
            }
        }

        Ok(AudioData {
            samples: resampled,
            sample_rate: output_rate as u32,
            channels: audio_data.channels,
        })
    }

    /// éŸ³é¢‘é¢„å¤„ç†
    fn preprocess_audio(&self, audio_data: &AudioData) -> Result<Tensor> {
        let audio_tensor = Tensor::from_slice(&audio_data.samples)
            .to_device(self.config.device)
            .to_kind(Kind::Float);

        // å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
        let max_val = audio_tensor.abs().max();
        let normalized = if max_val.double_value(&[]) > 0.0 {
            audio_tensor / max_val
        } else {
            audio_tensor
        };

        // æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        Ok(normalized.unsqueeze(0))
    }

    /// æå– HuBERT ç‰¹å¾
    fn extract_hubert_features(&self, audio: &Tensor) -> Result<Tensor> {
        // ä½¿ç”¨ HuBERT æå–ç‰¹å¾
        let hubert_output = self.hubert.extract_features(audio, None, false)?;
        Ok(hubert_output.last_hidden_state)
    }

    /// ä¼°è®¡ F0
    fn estimate_f0(&self, audio: &Tensor) -> Result<Vec<f64>> {
        let audio_vec: Vec<f32> = audio.squeeze_dim(0).try_into()?;
        let f0_result = self
            .f0_estimator
            .estimate(&audio_vec, self.config.f0_method)?;
        Ok(f0_result
            .frequencies
            .into_iter()
            .map(|x| x as f64)
            .collect())
    }

    /// F0 åå¤„ç†
    fn postprocess_f0(&self, mut f0_data: Vec<f64>) -> Result<Tensor> {
        // åº”ç”¨éŸ³é«˜è°ƒæ•´
        if self.config.pitch_shift != 1.0 {
            for f0 in &mut f0_data {
                if *f0 > 0.0 {
                    *f0 *= self.config.pitch_shift;
                }
            }
        }

        // ä¸­å€¼æ»¤æ³¢
        if self.config.f0_filter.median_filter_radius > 0 {
            f0_data = self.apply_median_filter(f0_data, self.config.f0_filter.median_filter_radius);
        }

        // F0 å¹³æ»‘
        if self.config.f0_filter.enable_smoothing {
            f0_data = self.apply_f0_smoothing(f0_data, self.config.f0_filter.smoothing_factor);
        }

        Ok(Tensor::from_slice(&f0_data).to_device(self.config.device))
    }

    /// åº”ç”¨ä¸­å€¼æ»¤æ³¢å™¨
    fn apply_median_filter(&self, data: Vec<f64>, radius: usize) -> Vec<f64> {
        let len = data.len();
        let mut filtered = data.clone();

        for i in 0..len {
            let start = i.saturating_sub(radius);
            let end = (i + radius + 1).min(len);

            let mut window: Vec<f64> = data[start..end].to_vec();
            window.sort_by(|a, b| a.partial_cmp(b).unwrap());

            filtered[i] = window[window.len() / 2];
        }

        filtered
    }

    /// åº”ç”¨ F0 å¹³æ»‘
    fn apply_f0_smoothing(&self, data: Vec<f64>, factor: f64) -> Vec<f64> {
        let mut smoothed = Vec::with_capacity(data.len());

        if !data.is_empty() {
            smoothed.push(data[0]);

            for i in 1..data.len() {
                let prev = smoothed[i - 1];
                let curr = data[i];

                // æŒ‡æ•°ç§»åŠ¨å¹³å‡
                let smooth_val = if curr > 0.0 && prev > 0.0 {
                    prev * factor + curr * (1.0 - factor)
                } else {
                    curr
                };

                smoothed.push(smooth_val);
            }
        }

        smoothed
    }

    /// ç‰¹å¾æ£€ç´¢
    fn retrieve_features(&self, features: &Tensor, index: &FaissIndex) -> Result<Tensor> {
        // å°†ç‰¹å¾è½¬æ¢ä¸ºæœç´¢æ ¼å¼
        let feature_vec: Vec<f32> = features.flatten(0, -1).try_into()?;

        // è½¬æ¢ä¸ºndarrayæ ¼å¼
        let feature_shape = features.size();
        let n_features = feature_shape[feature_shape.len() - 1] as usize;
        let feature_array = Array2::from_shape_vec((1, n_features), feature_vec)?;
        let feature_view: ArrayView2<f32> = feature_array.view();

        // æ‰§è¡Œ k-NN æœç´¢
        let k = 4; // æ£€ç´¢ top-k ä¸ªæœ€è¿‘é‚»
        let _search_results = index.search(feature_view, k)?;

        // æ··åˆåŸå§‹ç‰¹å¾å’Œæ£€ç´¢åˆ°çš„ç‰¹å¾
        // TODO: å®ç°ç‰¹å¾æ··åˆé€»è¾‘
        Ok(features.shallow_clone())
    }

    /// æ··åˆç‰¹å¾
    fn _blend_features(
        &self,
        original: &Tensor,
        _search_results: &[SearchResult],
    ) -> Result<Tensor> {
        // ç®€å•çš„åŠ æƒå¹³å‡æ··åˆ
        let mix_rate = self.config.index_rate;
        let original_weight = 1.0 - mix_rate;

        // è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ç‰¹å¾æ ¼å¼æ¥å®ç°æ··åˆé€»è¾‘
        // ç›®å‰è¿”å›åŸå§‹ç‰¹å¾ä½œä¸ºå ä½ç¬¦
        Ok(original * original_weight)
    }

    /// ç”ŸæˆéŸ³é¢‘
    fn generate_audio(&self, features: &Tensor, f0: &Tensor) -> Result<Tensor> {
        // ä½¿ç”¨ NSF-HiFiGAN ç”Ÿæˆå™¨ç”ŸæˆéŸ³é¢‘
        let generated = self.generator.forward(features, Some(f0), None)?;
        Ok(generated)
    }

    /// éŸ³é¢‘åå¤„ç†
    fn postprocess_audio(&self, audio_tensor: Tensor) -> Result<AudioData> {
        // ç§»é™¤æ‰¹æ¬¡ç»´åº¦å¹¶è½¬æ¢ä¸º Vec<f32>
        let audio_data: Vec<f32> = audio_tensor.squeeze_dim(0).try_into()?;

        // åº”ç”¨è½¯é™å¹…é˜²æ­¢å‰Šæ³¢
        let processed_data: Vec<f32> = audio_data
            .into_iter()
            .map(|x| {
                // Tanh è½¯é™å¹…
                if x.abs() > 0.95 {
                    x.signum() * 0.95 * (1.0 - (-x.abs() / 0.05).exp())
                } else {
                    x
                }
            })
            .collect();

        Ok(AudioData {
            samples: processed_data,
            sample_rate: self.config.target_sample_rate as u32,
            channels: 1,
        })
    }

    /// è·å–æ¨ç†ç»Ÿè®¡ä¿¡æ¯
    pub fn get_inference_stats(&self) -> InferenceStats {
        InferenceStats {
            device: format!("{:?}", self.config.device),
            hubert_parameters: self.count_hubert_parameters(),
            generator_parameters: self.count_generator_parameters(),
            has_index: self.index.is_some(),
            target_sample_rate: self.config.target_sample_rate,
        }
    }

    /// ç»Ÿè®¡ HuBERT å‚æ•°æ•°é‡
    fn count_hubert_parameters(&self) -> usize {
        // ç²—ç•¥ä¼°è®¡ï¼Œå®é™…åº”è¯¥éå†æ‰€æœ‰å‚æ•°
        1000000 // 1M å‚æ•°çš„å ä½ç¬¦
    }

    /// ç»Ÿè®¡ç”Ÿæˆå™¨å‚æ•°æ•°é‡
    fn count_generator_parameters(&self) -> usize {
        // ç²—ç•¥ä¼°è®¡ï¼Œå®é™…åº”è¯¥éå†æ‰€æœ‰å‚æ•°
        5000000 // 5M å‚æ•°çš„å ä½ç¬¦
    }
}

/// æ¨ç†ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug)]
pub struct InferenceStats {
    pub device: String,
    pub hubert_parameters: usize,
    pub generator_parameters: usize,
    pub has_index: bool,
    pub target_sample_rate: i64,
}

/// æ‰¹é‡æ¨ç†æ¥å£
pub struct BatchInference {
    inference_engine: RVCInference,
}

impl BatchInference {
    /// åˆ›å»ºæ‰¹é‡æ¨ç†å¼•æ“
    pub fn new(inference_engine: RVCInference) -> Self {
        Self { inference_engine }
    }

    /// æ‰¹é‡å¤„ç†éŸ³é¢‘æ–‡ä»¶
    pub fn process_batch(
        &self,
        input_files: &[impl AsRef<Path>],
        output_dir: impl AsRef<Path>,
    ) -> Result<Vec<AudioData>> {
        let mut results = Vec::new();

        for (i, input_file) in input_files.iter().enumerate() {
            println!(
                "ğŸ“ å¤„ç†æ–‡ä»¶ {}/{}: {:?}",
                i + 1,
                input_files.len(),
                input_file.as_ref()
            );

            let output_file = output_dir.as_ref().join(
                input_file
                    .as_ref()
                    .file_stem()
                    .unwrap_or_default()
                    .to_string_lossy()
                    .to_string()
                    + "_converted.wav",
            );

            match self
                .inference_engine
                .convert_voice(input_file, &output_file)
            {
                Ok(result) => {
                    results.push(result);
                    println!("âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {:?}", output_file);
                }
                Err(e) => {
                    println!("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {:?}, é”™è¯¯: {}", input_file.as_ref(), e);
                    return Err(e);
                }
            }
        }

        println!("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ! å…±å¤„ç† {} ä¸ªæ–‡ä»¶", results.len());
        Ok(results)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[test]
    fn test_inference_config_default() {
        let config = InferenceConfig::default();
        assert_eq!(config.speaker_id, 0);
        assert_eq!(config.f0_method, F0Method::Harvest);
        assert_eq!(config.pitch_shift, 1.0);
        assert_eq!(config.index_rate, 0.5);
    }

    #[test]
    fn test_f0_filter_default() {
        let filter_config = F0FilterConfig::default();
        assert_eq!(filter_config.median_filter_radius, 3);
        assert!(filter_config.enable_smoothing);
        assert_eq!(filter_config.smoothing_factor, 0.8);
    }

    #[test]
    fn test_audio_resampling() {
        // è¿™éœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„æ¨ç†å¼•æ“å®ä¾‹æ¥æµ‹è¯•
        // åœ¨å®é™…æµ‹è¯•ä¸­éœ€è¦æä¾›æ¨¡å‹æ–‡ä»¶è·¯å¾„
    }
}
