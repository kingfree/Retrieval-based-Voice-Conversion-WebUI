//! å®Œæ•´çš„éŸ³é¢‘å¤„ç†ç®¡é“
//!
//! è¯¥æ¨¡å—å®ç°äº†ç«¯åˆ°ç«¯çš„éŸ³é¢‘å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€ç‰¹å¾æå–ã€
//! è¯­éŸ³è½¬æ¢å’Œåå¤„ç†ç­‰æ­¥éª¤ã€‚æä¾›äº†é«˜çº§ API æ¥ç®€åŒ–éŸ³é¢‘è½¬æ¢æ“ä½œã€‚

use crate::{
    audio_utils::{AudioData, load_wav_simple, save_wav_simple},
    f0_estimation::{F0Config, F0Estimator},
    faiss_index::FaissIndex,
    generator::NSFHiFiGANGenerator,
    hubert::HuBERT,
    inference::InferenceConfig,
    model_loader::{ModelConfig as ModelLoaderConfig, ModelLoader},
};

use anyhow::{Result, anyhow};
use std::path::Path;
use std::time::Instant;
use tch::{Device, Tensor, nn};

/// éŸ³é¢‘å¤„ç†ç®¡é“é…ç½®
#[derive(Debug, Clone)]
pub struct AudioPipelineConfig {
    /// è¾“å…¥éŸ³é¢‘è·¯å¾„
    pub input_path: String,
    /// è¾“å‡ºéŸ³é¢‘è·¯å¾„
    pub output_path: String,
    /// æ¨¡å‹æ–‡ä»¶è·¯å¾„
    pub model_path: String,
    /// ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    pub index_path: Option<String>,
    /// æ¨ç†é…ç½®
    pub inference_config: InferenceConfig,
    /// éŸ³é¢‘é¢„å¤„ç†é…ç½®
    pub preprocessing: AudioPreprocessingConfig,
    /// éŸ³é¢‘åå¤„ç†é…ç½®
    pub postprocessing: AudioPostprocessingConfig,
}

/// éŸ³é¢‘é¢„å¤„ç†é…ç½®
#[derive(Debug, Clone)]
pub struct AudioPreprocessingConfig {
    /// æ˜¯å¦è¿›è¡ŒéŸ³é¢‘æ ‡å‡†åŒ–
    pub normalize: bool,
    /// æ˜¯å¦ç§»é™¤é™éŸ³
    pub remove_silence: bool,
    /// é™éŸ³é˜ˆå€¼
    pub silence_threshold: f32,
    /// æ˜¯å¦åº”ç”¨é¢„åŠ é‡
    pub preemphasis: bool,
    /// é¢„åŠ é‡ç³»æ•°
    pub preemphasis_coefficient: f32,
    /// ç›®æ ‡å“åº¦ (LUFS)
    pub target_lufs: Option<f32>,
}

/// éŸ³é¢‘åå¤„ç†é…ç½®
#[derive(Debug, Clone)]
pub struct AudioPostprocessingConfig {
    /// æ˜¯å¦åº”ç”¨å»åŠ é‡
    pub deemphasis: bool,
    /// å»åŠ é‡ç³»æ•°
    pub deemphasis_coefficient: f32,
    /// æ˜¯å¦åº”ç”¨è½¯é™å¹…
    pub apply_soft_clipping: bool,
    /// è½¯é™å¹…é˜ˆå€¼
    pub soft_clip_threshold: f32,
    /// æ˜¯å¦åº”ç”¨å™ªå£°é—¨é™
    pub apply_noise_gate: bool,
    /// å™ªå£°é—¨é™é˜ˆå€¼
    pub noise_gate_threshold: f32,
    /// è¾“å‡ºå¢ç›Š (dB)
    pub output_gain_db: f32,
}

/// å¤„ç†é˜¶æ®µ
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum ProcessingStage {
    Loading,
    Preprocessing,
    FeatureExtraction,
    F0Estimation,
    VoiceConversion,
    Postprocessing,
    Saving,
    Complete,
}

/// å¤„ç†è¿›åº¦ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct ProcessingProgress {
    /// å½“å‰é˜¶æ®µ
    pub stage: ProcessingStage,
    /// è¿›åº¦ç™¾åˆ†æ¯” (0-100)
    pub progress: f32,
    /// é˜¶æ®µæè¿°
    pub description: String,
    /// è€—æ—¶ (æ¯«ç§’)
    pub elapsed_ms: u64,
    /// æ˜¯å¦æœ‰é”™è¯¯
    pub has_error: bool,
    /// é”™è¯¯ä¿¡æ¯
    pub error_message: Option<String>,
}

/// è¿›åº¦å›è°ƒå‡½æ•°ç±»å‹
pub type ProgressCallback = Box<dyn Fn(ProcessingProgress) + Send + Sync>;

/// éŸ³é¢‘å¤„ç†ç®¡é“
pub struct AudioPipeline {
    config: AudioPipelineConfig,
    _model_config: ModelLoaderConfig,
    _vs: nn::VarStore,
    hubert: HuBERT,
    generator: NSFHiFiGANGenerator,
    f0_estimator: F0Estimator,
    index: Option<FaissIndex>,
    progress_callback: Option<ProgressCallback>,
    start_time: Instant,
}

impl AudioPipeline {
    /// åˆ›å»ºæ–°çš„éŸ³é¢‘å¤„ç†ç®¡é“
    pub fn new(config: AudioPipelineConfig) -> Result<Self> {
        let start_time = Instant::now();

        // åˆå§‹åŒ–è®¾å¤‡å’Œå˜é‡å­˜å‚¨
        let mut vs = nn::VarStore::new(config.inference_config.device);

        // åˆ›å»ºæ¨¡å‹åŠ è½½å™¨å¹¶åŠ è½½é…ç½®
        let model_loader = ModelLoader::new(config.inference_config.device);
        let model_config = Self::load_model_config(&config.model_path)?;

        // åŠ è½½æ¨¡å‹æƒé‡
        if Path::new(&config.model_path).exists() {
            println!("ğŸ“ åŠ è½½æ¨¡å‹æƒé‡: {}", config.model_path);
            match model_loader.load_pytorch_model(&config.model_path, &mut vs) {
                Ok(stats) => {
                    println!(
                        "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {:.1}M å‚æ•°",
                        stats.total_params as f64 / 1_000_000.0
                    );
                }
                Err(e) => {
                    println!("âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºæƒé‡: {}", e);
                }
            }
        }

        // åˆå§‹åŒ–ç»„ä»¶
        let hubert = Self::create_hubert(&vs, &model_config, config.inference_config.device)?;
        let generator = Self::create_generator(&vs, &model_config)?;
        let f0_estimator =
            Self::create_f0_estimator(&model_config, config.inference_config.device)?;

        // åŠ è½½ FAISS ç´¢å¼•
        let index = if let Some(ref index_path) = config.index_path {
            match FaissIndex::load(index_path) {
                Ok(idx) => {
                    println!("âœ… FAISS ç´¢å¼•åŠ è½½æˆåŠŸ");
                    Some(idx)
                }
                Err(e) => {
                    println!("âš ï¸  FAISS ç´¢å¼•åŠ è½½å¤±è´¥: {}", e);
                    None
                }
            }
        } else {
            None
        };

        Ok(Self {
            config,
            _model_config: model_config,
            _vs: vs,
            hubert,
            generator,
            f0_estimator,
            index,
            progress_callback: None,
            start_time,
        })
    }

    /// è®¾ç½®è¿›åº¦å›è°ƒ
    pub fn with_progress_callback(mut self, callback: ProgressCallback) -> Self {
        self.progress_callback = Some(callback);
        self
    }

    /// æ‰§è¡Œå®Œæ•´çš„éŸ³é¢‘å¤„ç†æµç¨‹
    pub async fn process(&mut self) -> Result<AudioData> {
        self.update_progress(ProcessingStage::Loading, 0.0, "å¼€å§‹å¤„ç†éŸ³é¢‘");

        // 1. åŠ è½½éŸ³é¢‘
        let mut audio_data = self.load_audio().await?;
        self.update_progress(ProcessingStage::Loading, 100.0, "éŸ³é¢‘åŠ è½½å®Œæˆ");

        // 2. é¢„å¤„ç†
        self.update_progress(ProcessingStage::Preprocessing, 0.0, "å¼€å§‹éŸ³é¢‘é¢„å¤„ç†");
        audio_data = self.preprocess_audio(audio_data).await?;
        self.update_progress(ProcessingStage::Preprocessing, 100.0, "éŸ³é¢‘é¢„å¤„ç†å®Œæˆ");

        // 3. ç‰¹å¾æå–
        self.update_progress(ProcessingStage::FeatureExtraction, 0.0, "æå– HuBERT ç‰¹å¾");
        let features = self.extract_features(&audio_data).await?;
        self.update_progress(ProcessingStage::FeatureExtraction, 100.0, "ç‰¹å¾æå–å®Œæˆ");

        // 4. F0 ä¼°è®¡
        self.update_progress(ProcessingStage::F0Estimation, 0.0, "ä¼°è®¡åŸºé¢‘ (F0)");
        let f0_data = self.estimate_f0(&audio_data).await?;
        self.update_progress(ProcessingStage::F0Estimation, 100.0, "F0 ä¼°è®¡å®Œæˆ");

        // 5. è¯­éŸ³è½¬æ¢
        self.update_progress(ProcessingStage::VoiceConversion, 0.0, "æ‰§è¡Œè¯­éŸ³è½¬æ¢");
        let converted_audio = self.perform_voice_conversion(&features, &f0_data).await?;
        self.update_progress(ProcessingStage::VoiceConversion, 100.0, "è¯­éŸ³è½¬æ¢å®Œæˆ");

        // 6. åå¤„ç†
        self.update_progress(ProcessingStage::Postprocessing, 0.0, "å¼€å§‹éŸ³é¢‘åå¤„ç†");
        let processed_audio = self.postprocess_audio(converted_audio).await?;
        self.update_progress(ProcessingStage::Postprocessing, 100.0, "éŸ³é¢‘åå¤„ç†å®Œæˆ");

        // 7. ä¿å­˜ç»“æœ
        self.update_progress(ProcessingStage::Saving, 0.0, "ä¿å­˜è¾“å‡ºéŸ³é¢‘");
        self.save_audio(&processed_audio).await?;
        self.update_progress(ProcessingStage::Saving, 100.0, "éŸ³é¢‘ä¿å­˜å®Œæˆ");

        self.update_progress(ProcessingStage::Complete, 100.0, "å¤„ç†å®Œæˆ");

        Ok(processed_audio)
    }

    /// åŠ è½½éŸ³é¢‘æ–‡ä»¶
    async fn load_audio(&self) -> Result<AudioData> {
        let audio_data =
            load_wav_simple(&self.config.input_path).map_err(|e| anyhow!("åŠ è½½éŸ³é¢‘å¤±è´¥: {}", e))?;

        println!("ğŸ“Š è¾“å…¥éŸ³é¢‘ä¿¡æ¯:");
        println!("   - æ–‡ä»¶: {}", self.config.input_path);
        println!("   - é‡‡æ ·ç‡: {}Hz", audio_data.sample_rate);
        println!(
            "   - æ—¶é•¿: {:.2}s",
            audio_data.samples.len() as f32 / audio_data.sample_rate as f32
        );
        println!("   - å£°é“æ•°: {}", audio_data.channels);

        Ok(audio_data)
    }

    /// éŸ³é¢‘é¢„å¤„ç†
    async fn preprocess_audio(&self, mut audio: AudioData) -> Result<AudioData> {
        let config = &self.config.preprocessing;

        // åº”ç”¨é¢„åŠ é‡
        if config.preemphasis {
            audio = self.apply_preemphasis(audio, config.preemphasis_coefficient)?;
        }

        // ç§»é™¤é™éŸ³
        if config.remove_silence {
            audio = self.remove_silence(audio, config.silence_threshold)?;
        }

        // éŸ³é¢‘æ ‡å‡†åŒ–
        if config.normalize {
            audio = self.normalize_audio(audio)?;
        }

        // å“åº¦æ ‡å‡†åŒ–
        if let Some(target_lufs) = config.target_lufs {
            audio = self.normalize_loudness(audio, target_lufs)?;
        }

        // é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if audio.sample_rate != self.config.inference_config.target_sample_rate as u32 {
            audio = self.resample_audio(audio)?;
        }

        Ok(audio)
    }

    /// æå– HuBERT ç‰¹å¾
    async fn extract_features(&self, audio: &AudioData) -> Result<Tensor> {
        let audio_tensor = Tensor::from_slice(&audio.samples)
            .to_device(self.config.inference_config.device)
            .unsqueeze(0); // æ·»åŠ æ‰¹æ¬¡ç»´åº¦

        let output = self.hubert.extract_features(&audio_tensor, None, false)?;
        Ok(output.last_hidden_state)
    }

    /// ä¼°è®¡ F0
    async fn estimate_f0(&self, audio: &AudioData) -> Result<Tensor> {
        let f0_result = self
            .f0_estimator
            .estimate(&audio.samples, self.config.inference_config.f0_method)?;

        // åº”ç”¨éŸ³è°ƒè°ƒæ•´
        let mut f0_values: Vec<f64> = f0_result
            .frequencies
            .into_iter()
            .map(|f| {
                if f > 0.0 {
                    f as f64 * self.config.inference_config.pitch_shift
                } else {
                    0.0
                }
            })
            .collect();

        // åº”ç”¨ F0 æ»¤æ³¢
        f0_values = self.apply_f0_filtering(f0_values)?;

        Ok(Tensor::from_slice(&f0_values).to_device(self.config.inference_config.device))
    }

    /// æ‰§è¡Œè¯­éŸ³è½¬æ¢
    async fn perform_voice_conversion(&self, features: &Tensor, f0: &Tensor) -> Result<AudioData> {
        // å¦‚æœæœ‰ FAISS ç´¢å¼•ï¼Œè¿›è¡Œç‰¹å¾æ£€ç´¢
        let enhanced_features = if let Some(ref index) = self.index {
            self.enhance_features_with_retrieval(features, index)?
        } else {
            features.shallow_clone()
        };

        // ç”ŸæˆéŸ³é¢‘
        let output_tensor = self.generator.forward(&enhanced_features, Some(f0), None)?;

        // è½¬æ¢ä¸ºéŸ³é¢‘æ•°æ®
        let output_samples: Vec<f32> = output_tensor
            .try_into()
            .map_err(|e| anyhow!("å¼ é‡è½¬æ¢å¤±è´¥: {:?}", e))?;

        Ok(AudioData {
            samples: output_samples,
            sample_rate: self.config.inference_config.target_sample_rate as u32,
            channels: 1,
        })
    }

    /// éŸ³é¢‘åå¤„ç†
    async fn postprocess_audio(&self, mut audio: AudioData) -> Result<AudioData> {
        let config = &self.config.postprocessing;

        // åº”ç”¨å»åŠ é‡
        if config.deemphasis {
            audio = self.apply_deemphasis(audio, config.deemphasis_coefficient)?;
        }

        // åº”ç”¨è½¯é™å¹…
        if config.apply_soft_clipping {
            audio = self.apply_soft_clipping(audio, config.soft_clip_threshold)?;
        }

        // åº”ç”¨å™ªå£°é—¨é™
        if config.apply_noise_gate {
            audio = self.apply_noise_gate(audio, config.noise_gate_threshold)?;
        }

        // åº”ç”¨è¾“å‡ºå¢ç›Š
        if config.output_gain_db != 0.0 {
            audio = self.apply_gain(audio, config.output_gain_db)?;
        }

        Ok(audio)
    }

    /// ä¿å­˜éŸ³é¢‘æ–‡ä»¶
    async fn save_audio(&self, audio: &AudioData) -> Result<()> {
        save_wav_simple(&self.config.output_path, audio)
            .map_err(|e| anyhow!("ä¿å­˜éŸ³é¢‘å¤±è´¥: {}", e))?;

        println!("ğŸ’¾ è¾“å‡ºéŸ³é¢‘å·²ä¿å­˜åˆ°: {}", self.config.output_path);
        Ok(())
    }

    /// åº”ç”¨é¢„åŠ é‡
    fn apply_preemphasis(&self, mut audio: AudioData, coefficient: f32) -> Result<AudioData> {
        if !audio.samples.is_empty() {
            for i in (1..audio.samples.len()).rev() {
                audio.samples[i] -= coefficient * audio.samples[i - 1];
            }
        }
        Ok(audio)
    }

    /// åº”ç”¨å»åŠ é‡
    fn apply_deemphasis(&self, mut audio: AudioData, coefficient: f32) -> Result<AudioData> {
        if !audio.samples.is_empty() {
            for i in 1..audio.samples.len() {
                audio.samples[i] += coefficient * audio.samples[i - 1];
            }
        }
        Ok(audio)
    }

    /// ç§»é™¤é™éŸ³
    fn remove_silence(&self, audio: AudioData, threshold: f32) -> Result<AudioData> {
        let mut result_samples = Vec::new();
        let window_size = 1024;
        let hop_size = 512;

        for start in (0..audio.samples.len()).step_by(hop_size) {
            let end = (start + window_size).min(audio.samples.len());
            let window = &audio.samples[start..end];

            // è®¡ç®—çª—å£çš„ RMS
            let rms = (window.iter().map(|x| x * x).sum::<f32>() / window.len() as f32).sqrt();

            // å¦‚æœ RMS è¶…è¿‡é˜ˆå€¼ï¼Œä¿ç•™è¯¥çª—å£
            if rms > threshold {
                result_samples.extend_from_slice(window);
            }
        }

        Ok(AudioData {
            samples: result_samples,
            ..audio
        })
    }

    /// éŸ³é¢‘æ ‡å‡†åŒ–
    fn normalize_audio(&self, mut audio: AudioData) -> Result<AudioData> {
        if let Some(max_val) = audio.samples.iter().map(|x| x.abs()).fold(None, |acc, x| {
            Some(match acc {
                Some(y) => x.max(y),
                None => x,
            })
        }) {
            if max_val > 0.0 {
                let scale = 0.95 / max_val; // ç•™ä¸€äº›ä½™é‡é¿å…å‰Šæ³¢
                for sample in &mut audio.samples {
                    *sample *= scale;
                }
            }
        }
        Ok(audio)
    }

    /// å“åº¦æ ‡å‡†åŒ–
    fn normalize_loudness(&self, audio: AudioData, target_lufs: f32) -> Result<AudioData> {
        // ç®€åŒ–çš„å“åº¦è®¡ç®—ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ ITU-R BS.1770 æ ‡å‡†ï¼‰
        let rms =
            (audio.samples.iter().map(|x| x * x).sum::<f32>() / audio.samples.len() as f32).sqrt();
        let current_lufs = 20.0 * rms.log10(); // ç®€åŒ–è®¡ç®—

        let gain_db = target_lufs - current_lufs;
        self.apply_gain(audio, gain_db)
    }

    /// åº”ç”¨å¢ç›Š
    fn apply_gain(&self, mut audio: AudioData, gain_db: f32) -> Result<AudioData> {
        let gain_linear = 10.0_f32.powf(gain_db / 20.0);
        for sample in &mut audio.samples {
            *sample *= gain_linear;
        }
        Ok(audio)
    }

    /// åº”ç”¨è½¯é™å¹…
    fn apply_soft_clipping(&self, mut audio: AudioData, threshold: f32) -> Result<AudioData> {
        for sample in &mut audio.samples {
            let abs_val = sample.abs();
            if abs_val > threshold {
                *sample =
                    sample.signum() * threshold * (1.0 - (-((abs_val - threshold) / 0.1)).exp());
            }
        }
        Ok(audio)
    }

    /// åº”ç”¨å™ªå£°é—¨é™
    fn apply_noise_gate(&self, mut audio: AudioData, threshold: f32) -> Result<AudioData> {
        let window_size = 1024;
        for chunk in audio.samples.chunks_mut(window_size) {
            let rms = (chunk.iter().map(|x| x * x).sum::<f32>() / chunk.len() as f32).sqrt();
            if rms < threshold {
                for sample in chunk {
                    *sample *= rms / threshold; // è¡°å‡è€Œä¸æ˜¯å®Œå…¨é™éŸ³
                }
            }
        }
        Ok(audio)
    }

    /// é‡é‡‡æ ·éŸ³é¢‘
    fn resample_audio(&self, audio: AudioData) -> Result<AudioData> {
        let target_rate = self.config.inference_config.target_sample_rate as f32;
        let ratio = target_rate / audio.sample_rate as f32;

        let new_length = (audio.samples.len() as f32 * ratio) as usize;
        let mut resampled = Vec::with_capacity(new_length);

        // ç®€å•çº¿æ€§æ’å€¼é‡é‡‡æ ·
        for i in 0..new_length {
            let src_index = i as f32 / ratio;
            let idx = src_index as usize;

            if idx + 1 < audio.samples.len() {
                let frac = src_index - idx as f32;
                let sample = audio.samples[idx] * (1.0 - frac) + audio.samples[idx + 1] * frac;
                resampled.push(sample);
            } else if idx < audio.samples.len() {
                resampled.push(audio.samples[idx]);
            }
        }

        Ok(AudioData {
            samples: resampled,
            sample_rate: target_rate as u32,
            channels: audio.channels,
        })
    }

    /// åº”ç”¨ F0 æ»¤æ³¢
    fn apply_f0_filtering(&self, mut f0_values: Vec<f64>) -> Result<Vec<f64>> {
        let config = &self.config.inference_config.f0_filter;

        // ä¸­å€¼æ»¤æ³¢
        if config.median_filter_radius > 0 {
            f0_values = self.median_filter(f0_values, config.median_filter_radius);
        }

        // å¹³æ»‘æ»¤æ³¢
        if config.enable_smoothing {
            f0_values = self.smooth_f0(f0_values, config.smoothing_factor);
        }

        Ok(f0_values)
    }

    /// ä¸­å€¼æ»¤æ³¢
    fn median_filter(&self, data: Vec<f64>, radius: usize) -> Vec<f64> {
        let mut filtered = data.clone();
        let len = data.len();

        for i in 0..len {
            let start = i.saturating_sub(radius);
            let end = (i + radius + 1).min(len);

            let mut window: Vec<f64> = data[start..end].to_vec();
            window.sort_by(|a, b| a.partial_cmp(b).unwrap());

            filtered[i] = window[window.len() / 2];
        }

        filtered
    }

    /// F0 å¹³æ»‘
    fn smooth_f0(&self, data: Vec<f64>, factor: f64) -> Vec<f64> {
        let mut smoothed = Vec::with_capacity(data.len());

        if !data.is_empty() {
            smoothed.push(data[0]);

            for i in 1..data.len() {
                let prev = smoothed[i - 1];
                let curr = data[i];

                let smooth_val = if curr > 0.0 && prev > 0.0 {
                    prev * factor + curr * (1.0 - factor)
                } else {
                    curr
                };

                smoothed.push(smooth_val);
            }
        }

        smoothed
    }

    /// ä½¿ç”¨æ£€ç´¢å¢å¼ºç‰¹å¾
    fn enhance_features_with_retrieval(
        &self,
        features: &Tensor,
        _index: &FaissIndex,
    ) -> Result<Tensor> {
        // ç®€åŒ–çš„ç‰¹å¾æ£€ç´¢å®ç°
        // å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„ç‰¹å¾æ··åˆé€»è¾‘
        let mix_rate = self.config.inference_config.index_rate;
        let enhanced = features * (1.0 - mix_rate);
        Ok(enhanced)
    }

    /// æ›´æ–°è¿›åº¦
    fn update_progress(&self, stage: ProcessingStage, progress: f32, description: &str) {
        if let Some(ref callback) = self.progress_callback {
            let progress_info = ProcessingProgress {
                stage,
                progress,
                description: description.to_string(),
                elapsed_ms: self.start_time.elapsed().as_millis() as u64,
                has_error: false,
                error_message: None,
            };
            callback(progress_info);
        }
    }

    /// åŠ è½½æ¨¡å‹é…ç½®
    fn load_model_config(model_path: &str) -> Result<ModelLoaderConfig> {
        let config_path = Path::new(model_path).with_extension("json");

        if config_path.exists() {
            ModelLoader::load_config(&config_path)
        } else {
            Ok(ModelLoaderConfig::default())
        }
    }

    /// åˆ›å»º HuBERT å®ä¾‹
    fn create_hubert(
        vs: &nn::VarStore,
        model_config: &ModelLoaderConfig,
        device: Device,
    ) -> Result<HuBERT> {
        let hubert_config = crate::hubert::HuBERTConfig {
            feature_dim: model_config.feature_dim,
            encoder_layers: model_config.hubert.encoder_layers,
            encoder_attention_heads: model_config.hubert.attention_heads,
            encoder_ffn_embed_dim: model_config.hubert.ffn_dim,
            dropout: model_config.hubert.dropout,
            ..Default::default()
        };

        Ok(HuBERT::new(&vs.root(), hubert_config, device))
    }

    /// åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    fn create_generator(
        vs: &nn::VarStore,
        model_config: &ModelLoaderConfig,
    ) -> Result<NSFHiFiGANGenerator> {
        let generator_config = crate::generator::GeneratorConfig {
            input_dim: model_config.generator.input_dim,
            upsample_rates: model_config.generator.upsample_rates.clone(),
            upsample_kernel_sizes: model_config.generator.upsample_kernel_sizes.clone(),
            resblock_kernel_sizes: model_config.generator.resblock_kernel_sizes.clone(),
            resblock_dilation_sizes: model_config.generator.resblock_dilation_sizes.clone(),
            leaky_relu_slope: model_config.generator.leaky_relu_slope,
            use_nsf: model_config.generator.use_nsf,
            ..Default::default()
        };

        Ok(NSFHiFiGANGenerator::new(&vs.root(), generator_config))
    }

    /// åˆ›å»º F0 ä¼°è®¡å™¨å®ä¾‹
    fn create_f0_estimator(
        model_config: &ModelLoaderConfig,
        device: Device,
    ) -> Result<F0Estimator> {
        let f0_config = F0Config {
            f0_min: model_config.f0_config.f0_min,
            f0_max: model_config.f0_config.f0_max,
            ..Default::default()
        };

        Ok(F0Estimator::new(f0_config, device))
    }
}

impl Default for AudioPreprocessingConfig {
    fn default() -> Self {
        Self {
            normalize: true,
            remove_silence: false,
            silence_threshold: 0.01,
            preemphasis: true,
            preemphasis_coefficient: 0.97,
            target_lufs: Some(-23.0), // EBU R128 æ ‡å‡†
        }
    }
}

impl Default for AudioPostprocessingConfig {
    fn default() -> Self {
        Self {
            deemphasis: true,
            deemphasis_coefficient: 0.97,
            apply_soft_clipping: true,
            soft_clip_threshold: 0.95,
            apply_noise_gate: false,
            noise_gate_threshold: 0.001,
            output_gain_db: 0.0,
        }
    }
}

/// ä¾¿æ·çš„éŸ³é¢‘å¤„ç†å‡½æ•°
pub mod utils {
    use super::*;

    /// ç®€å•çš„éŸ³é¢‘è½¬æ¢
    pub async fn convert_audio_simple(
        input_path: &str,
        output_path: &str,
        model_path: &str,
    ) -> Result<()> {
        let config = AudioPipelineConfig {
            input_path: input_path.to_string(),
            output_path: output_path.to_string(),
            model_path: model_path.to_string(),
            index_path: None,
            inference_config: InferenceConfig::default(),
            preprocessing: AudioPreprocessingConfig::default(),
            postprocessing: AudioPostprocessingConfig::default(),
        };

        let mut pipeline = AudioPipeline::new(config)?;
        pipeline.process().await?;

        Ok(())
    }

    /// å¸¦è¿›åº¦å›è°ƒçš„éŸ³é¢‘è½¬æ¢
    pub async fn convert_audio_with_progress(
        input_path: &str,
        output_path: &str,
        model_path: &str,
        progress_callback: ProgressCallback,
    ) -> Result<()> {
        let config = AudioPipelineConfig {
            input_path: input_path.to_string(),
            output_path: output_path.to_string(),
            model_path: model_path.to_string(),
            index_path: None,
            inference_config: InferenceConfig::default(),
            preprocessing: AudioPreprocessingConfig::default(),
            postprocessing: AudioPostprocessingConfig::default(),
        };

        let mut pipeline = AudioPipeline::new(config)?.with_progress_callback(progress_callback);
        pipeline.process().await?;

        Ok(())
    }

    /// æ‰¹é‡éŸ³é¢‘è½¬æ¢
    pub async fn batch_convert_audio(
        input_files: &[String],
        output_dir: &str,
        model_path: &str,
        _progress_callback: Option<ProgressCallback>,
    ) -> Result<Vec<String>> {
        let mut output_files = Vec::new();

        for (i, input_file) in input_files.iter().enumerate() {
            let input_path = Path::new(input_file);
            let file_stem = input_path.file_stem().unwrap().to_string_lossy();
            let output_file = format!("{}/{}_converted.wav", output_dir, file_stem);

            println!("å¤„ç†æ–‡ä»¶ {}/{}: {}", i + 1, input_files.len(), input_file);

            let config = AudioPipelineConfig {
                input_path: input_file.clone(),
                output_path: output_file.clone(),
                model_path: model_path.to_string(),
                index_path: None,
                inference_config: InferenceConfig::default(),
                preprocessing: AudioPreprocessingConfig::default(),
                postprocessing: AudioPostprocessingConfig::default(),
            };

            let mut pipeline = AudioPipeline::new(config)?;

            pipeline.process().await?;
            output_files.push(output_file);
        }

        Ok(output_files)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tch::Device;

    #[test]
    fn test_audio_pipeline_config_default() {
        let preprocessing = AudioPreprocessingConfig::default();
        assert!(preprocessing.normalize);
        assert!(preprocessing.preemphasis);
        assert_eq!(preprocessing.preemphasis_coefficient, 0.97);

        let postprocessing = AudioPostprocessingConfig::default();
        assert!(postprocessing.deemphasis);
        assert!(postprocessing.apply_soft_clipping);
        assert_eq!(postprocessing.output_gain_db, 0.0);
    }

    #[test]
    fn test_processing_stage() {
        let stage = ProcessingStage::Loading;
        assert_eq!(stage, ProcessingStage::Loading);
        assert_ne!(stage, ProcessingStage::Complete);
    }

    #[tokio::test]
    async fn test_audio_pipeline_creation() {
        let config = AudioPipelineConfig {
            input_path: "test.wav".to_string(),
            output_path: "output.wav".to_string(),
            model_path: "model.pth".to_string(),
            index_path: None,
            inference_config: InferenceConfig {
                device: Device::Cpu,
                ..Default::default()
            },
            preprocessing: AudioPreprocessingConfig::default(),
            postprocessing: AudioPostprocessingConfig::default(),
        };

        // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•ä¼šå¤±è´¥ï¼Œå› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨
        // ä½†å®ƒéªŒè¯äº†é…ç½®ç»“æ„çš„æ­£ç¡®æ€§
        let result = AudioPipeline::new(config);
        assert!(result.is_err()); // é¢„æœŸå¤±è´¥ï¼Œå› ä¸ºæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨
    }
}
